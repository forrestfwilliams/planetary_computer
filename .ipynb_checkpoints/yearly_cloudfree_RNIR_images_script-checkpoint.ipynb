{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentinel 2 Cloudless Mosaic\n",
    "\n",
    "This tutorial constructs a *cloudless mosaic* (also known as a composite) from a time series of [Sentinel-2 Level-2A](https://planetarycomputer.microsoft.com/dataset/sentinel-2-l2a) images and is modified from the example notebook provided by Microsoft. This notebook performs the following steps:\n",
    "\n",
    "* Find a time series of images within a bounding box\n",
    "* Stack those images together into a single array\n",
    "* Mask clouds and cloud shadows\n",
    "* Synthesize a panchromatic band by averageing Red, Green, Blue and NIR bands\n",
    "* Compute the cloudless mosaic by taking a median\n",
    "* Save the result to a GeoTiff\n",
    "\n",
    "This notebook is designed for the processing of large areas, so tasks like plotting that are useful but resource-intensive are omitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "import rasterio.features\n",
    "import rioxarray\n",
    "import stackstac\n",
    "import pystac_client\n",
    "import planetary_computer\n",
    "\n",
    "import pyproj\n",
    "from shapely.ops import transform\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import xrspatial.multispectral as ms\n",
    "\n",
    "import dask\n",
    "from dask_gateway import GatewayCluster\n",
    "from dask import visualize\n",
    "\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dask cluster\n",
    "\n",
    "We're going to process a large amount of data. To cut down on the execution time, we'll use a Dask cluster to do the computation in parallel, adaptively scaling to add and remove workers as needed. See [Scale With Dask](../quickstarts/scale-with-dask.ipynb) for more on using Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the cluster\n",
    "cluster = GatewayCluster()  # Creates the Dask Scheduler. Might take a minute.\n",
    "client = cluster.get_client()\n",
    "cluster.adapt(minimum=4, maximum=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pccompute.westeurope.cloudapp.azure.com/compute/services/dask-gateway/clusters/prod.745dee1dd9114fc4886b880402056a77/status\n"
     ]
    }
   ],
   "source": [
    "print(cluster.dashboard_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover data\n",
    "\n",
    "In this step we define our bounding box by creating a Shapely Polygon object. The Polygon object is created from a set of coordinate pairs in **Latitude and Longitude** (epsg 3857). A simple way of getting the coordinate pairs is by creating a bounding box in Google Earth, saving it to a kml, then opening it as a text file and copying the coordinates.\n",
    "\n",
    "At this point, you'll have to decide if you want to process multiple years at once, or if you want to process the years separately. This decision comes down to how much compute power you have access to. For the Dask cluster parameters specified, `cluster.adapt(minimum=4, maximum=24)`, the maximum amount of images used should be less than 200. You can alter the number of images used by changing the values of `date_range`, `max_cloud`, and `pol`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proroa: R129, T60GUA\n",
    "#andrew: R129, T60GVA\n",
    "\n",
    "# setting options\n",
    "date_range = '2016-01-01/2022-01-01'\n",
    "frame = 'T60GVA'\n",
    "orbit = 'R129'\n",
    "max_cloud_image = 33\n",
    "max_cloud_bbox = 5\n",
    "\n",
    "# get bounding box (minx, miny, maxx, maxy)\n",
    "gdf = gpd.read_file('andrew.geojson')\n",
    "bbox = tuple(gdf.total_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `pystac_client` we can search the Planetary Computer's STAC endpoint for items matching our query parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stac = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "\n",
    "search = stac.search(\n",
    "    bbox=bbox,\n",
    "    datetime=date_range,\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    limit=500,  # fetch items in batches of 500\n",
    "    query={\"eo:cloud_cover\": {\"gte\":0,\"lte\": max_cloud_image}},\n",
    ")\n",
    "\n",
    "# Get items with the correct relative orbit\n",
    "items = list(search.get_items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we restrict the results to the same orbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images before cloud masking is 117\n"
     ]
    }
   ],
   "source": [
    "# grab ids\n",
    "ids = np.array([x.id.split('_') for x in items])\n",
    "\n",
    "# get correct orbit\n",
    "ids = ids[ids[:,3] == orbit]\n",
    "\n",
    "# get correct frame\n",
    "ids = ids[ids[:,4] == frame]\n",
    "\n",
    "# grab valid ids\n",
    "valid_ids = ['_'.join(x) for x in ids]\n",
    "\n",
    "#subset items\n",
    "items = [x for x in items if any([y == x.id for y in valid_ids])]\n",
    "\n",
    "print(f'Number of images before cloud masking is {len(items)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the year, this should return about 100-150 images for our study area over space, time, and cloudiness. Those items will still have *some* clouds over portions of the scenes, though. To create our cloudless mosaic, we'll load the data into an [xarray](https://xarray.pydata.org/en/stable/) DataArray using [stackstac](https://stackstac.readthedocs.io/) and then reduce the time-series of images down to a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "signed_items = []\n",
    "for item in items:\n",
    "    item.clear_links()\n",
    "    signed_items.append(planetary_computer.sign(item).to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we load the data and perform some initial cleaing that includes:\n",
    "* subsetting to our exact bounding box\n",
    "* removing pixels that correspond clouds and clouds shadows\n",
    "\n",
    "To perform our cloud masking, we use Sentinel-2's Scene Classification Layer ([SCL](https://sentinels.copernicus.eu/web/sentinel/technical-guides/sentinel-2-msi/level-2a/algorithm)) and mask out values 3, 8, 9, and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\n",
    "    stackstac.stack(\n",
    "        signed_items,\n",
    "        assets=[\"B08\",\"SCL\"],\n",
    "        chunksize=4096,\n",
    "        resolution=10\n",
    "    )\n",
    "    .where(lambda x: x > 0, other=np.nan)  # sentinel-2 uses 0 as nodata\n",
    ")\n",
    "\n",
    "# Get bounding box in projection of data\n",
    "minx, miny, maxx, maxy = tuple(gdf.to_crs(data.crs).total_bounds)\n",
    "\n",
    "# Subset data and mask clouds\n",
    "data = data.sel(x=slice(minx, maxx), y=slice(maxy,miny))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = data.groupby('time').first(skipna=False)\n",
    "valid = xr.where(first.sel(band='SCL',drop=True).isin([3,8,9]),x=0,y=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_valid = valid.sum(dim=['x','y']).compute().to_numpy() / (data.shape[2] * data.shape[3])\n",
    "pct_valid = pct_valid.squeeze()\n",
    "dates = valid.time.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clouds = pd.DataFrame({'date':dates,'pct_valid':pct_valid[:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = clouds.loc[clouds.pct_valid > (100-max_cloud_bbox)/100].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "best['year'] = best.date.dt.year\n",
    "counts = best.groupby('year').count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2016 contains 6 images\n",
      "Year 2017 contains 15 images\n",
      "Year 2018 contains 11 images\n",
      "Year 2019 contains 13 images\n",
      "Year 2020 contains 15 images\n",
      "Year 2021 contains 12 images\n"
     ]
    }
   ],
   "source": [
    "for i,row in counts.iterrows():\n",
    "    print(f'Year {row[\"year\"]} contains {row[\"date\"]} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dates = data.sel(band=[\"B08\"],time=list(best.date)).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max NCC summarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104, 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date1,date2 = zip(*[sorted(x) for x in itertools.combinations(best_dates.time.to_numpy(),2)])\n",
    "df_ncc = pd.DataFrame({'date1':date1,'date2':date2})\n",
    "df_ncc['date_diff'] = (df_ncc.date2 - df_ncc.date1).dt.days\n",
    "df_ncc = df_ncc.loc[(df_ncc.date_diff >= 345) & (df_ncc.date_diff <= 380)].copy()\n",
    "\n",
    "df_ncc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_ncc = best_dates.drop((set(best_dates.coords)  - set(best_dates.dims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_output = []\n",
    "\n",
    "for i,row in df_ncc.iterrows():\n",
    "    data1 = for_ncc.sel(time=row['date1'],drop=True)\n",
    "    data2 = for_ncc.sel(time=row['date2'],drop=True)\n",
    "    lazy_output.append([row['date1'],row['date2'],xr.corr(data1,data2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f12b17b78b44ef990d3fd09fd4020e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 3s, sys: 16.8 s, total: 1min 20s\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_ncc['ncc'] = 0\n",
    "for date1,date2,arr in tqdm(lazy_output):\n",
    "    df_ncc.loc[(df_ncc.date1==date1)&(df_ncc.date2==date2),'ncc'] = arr.values.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = list(set(df_ncc.date1) | set(df_ncc.date2))\n",
    "dates.sort()\n",
    "\n",
    "box_ncc = pd.DataFrame(index=dates,columns=dates,dtype=np.float64)\n",
    "for i,row in df_ncc.iterrows():\n",
    "    date1 = row['date1']\n",
    "    date2 = row['date2']\n",
    "    box_ncc.loc[box_ncc.index==date1,date2] = row['ncc']\n",
    "    box_ncc.loc[box_ncc.index==date2,date1] = row['ncc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ncc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>2016-11-22 22:16:02.026</td>\n",
       "      <td>0.865657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>2017-11-22 22:15:49.027</td>\n",
       "      <td>0.884074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>2018-10-23 22:16:01.024</td>\n",
       "      <td>0.864480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>2019-11-02 22:16:09.024</td>\n",
       "      <td>0.900378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>2020-11-11 22:16:11.024</td>\n",
       "      <td>0.911931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>2021-09-02 22:15:59.024</td>\n",
       "      <td>0.909412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date       ncc\n",
       "year                                  \n",
       "2016 2016-11-22 22:16:02.026  0.865657\n",
       "2017 2017-11-22 22:15:49.027  0.884074\n",
       "2018 2018-10-23 22:16:01.024  0.864480\n",
       "2019 2019-11-02 22:16:09.024  0.900378\n",
       "2020 2020-11-11 22:16:11.024  0.911931\n",
       "2021 2021-09-02 22:15:59.024  0.909412"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_mean = box_ncc.mean(axis=0).reset_index().rename(columns={'index':'date',0:'ncc'})\n",
    "date_mean['year'] = date_mean.date.dt.year\n",
    "max_ncc = date_mean.groupby('year').max()\n",
    "max_ncc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing s2_l2_R129_T60GVA_20161122.tif...\n",
      "Writing s2_l2_R129_T60GVA_20171122.tif...\n",
      "Writing s2_l2_R129_T60GVA_20181023.tif...\n",
      "Writing s2_l2_R129_T60GVA_20191102.tif...\n",
      "Writing s2_l2_R129_T60GVA_20201111.tif...\n",
      "Writing s2_l2_R129_T60GVA_20210902.tif...\n"
     ]
    }
   ],
   "source": [
    "for t in max_ncc.date:\n",
    "    time_name = datetime.strftime(t,'%Y%m%d')\n",
    "    name = f's2_l2_{orbit}_{frame}_{time_name}.tif'\n",
    "    print(f'Writing {name}...')\n",
    "    best_dates.sel(time=t).rio.to_raster(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median summarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = best_dates.groupby('time.year').median().fillna(0).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median image processing complete\n"
     ]
    }
   ],
   "source": [
    "median = median.rio.write_crs(pyproj.CRS(data.crs).to_string()).drop('proj:bbox')\n",
    "median = median.rename('Median_NIR')\n",
    "print('Median image processing complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing s2_l2_R129_T60GVA_20160601.tif...\n",
      "Writing s2_l2_R129_T60GVA_20170601.tif...\n",
      "Writing s2_l2_R129_T60GVA_20180601.tif...\n",
      "Writing s2_l2_R129_T60GVA_20190601.tif...\n",
      "Writing s2_l2_R129_T60GVA_20200601.tif...\n",
      "Writing s2_l2_R129_T60GVA_20210601.tif...\n"
     ]
    }
   ],
   "source": [
    "for y in median.year.to_numpy():\n",
    "    name = f's2_l2_{orbit}_{frame}_{y}0601.tif'\n",
    "    print(f'Writing {name}...')\n",
    "    median.sel(year=y).rio.to_raster(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed successfully!\n"
     ]
    }
   ],
   "source": [
    "print('Completed successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close cluster\n",
    "Once we're done with our processing, let's be a good steward of our resources and close our cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "And you're done! The completed GeoTiff files should be in the same directory as this notebook, and can be downloaded via Jupyter's GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2ca0804b9f904dab815db80637a4f2d9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e2f3ac516e3b4cf3a1ba1fc6aa0897ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "layout": "IPY_MODEL_2ca0804b9f904dab815db80637a4f2d9"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
